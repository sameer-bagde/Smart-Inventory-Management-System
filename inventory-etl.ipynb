{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9881682,"sourceType":"datasetVersion","datasetId":6067526}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install mysql-connector-python\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T12:10:52.921975Z","iopub.execute_input":"2024-11-12T12:10:52.922807Z","iopub.status.idle":"2024-11-12T12:11:10.231022Z","shell.execute_reply.started":"2024-11-12T12:10:52.922757Z","shell.execute_reply":"2024-11-12T12:11:10.229670Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting mysql-connector-python\n  Downloading mysql_connector_python-9.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.0 kB)\nDownloading mysql_connector_python-9.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (34.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: mysql-connector-python\nSuccessfully installed mysql-connector-python-9.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**1.Establish Connection with MySQL**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport mysql.connector\nfrom datetime import datetime\n\n# 1. Create database connection\ndef create_db_connection():\n    try:\n        db = mysql.connector.connect(\n            host=\"mysql-204e2163-bagdesameer92-369f.d.aivencloud.com\",\n            user=\"avnadmin\",\n            password=\"AVNS__zNcky48VFw4ThNwd6c\",\n            database=\"defaultdb\",\n            port=21095,\n            ssl_ca='/kaggle/input/interview/ca.pem'  # CA cert path for SSL connection\n        )\n        return db\n    except mysql.connector.Error as err:\n        print(f\"Error connecting to database: {err}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:20:36.461611Z","iopub.execute_input":"2024-11-12T10:20:36.462050Z","iopub.status.idle":"2024-11-12T10:20:36.470103Z","shell.execute_reply.started":"2024-11-12T10:20:36.462009Z","shell.execute_reply":"2024-11-12T10:20:36.468682Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"**2. Extract: Data Collection**\n\nextract_data(db):\n\nThis part of the ETL process is responsible for extracting the raw data from a source (in this case, MySQL database tables).\n\nThe extract_data function connects to the database and retrieves data from five tables: ProductInformation, InventoryDetails, Pricing, SalesMetrics, and ReplenishmentInformation.\nEach table is queried using SQL, and the results are stored in pandas DataFrames for further processing.\nThe function uses the pd.read_sql method to directly load the data from the SQL queries into pandas DataFrames.\nIf there's an error in extraction, the function returns None.","metadata":{}},{"cell_type":"code","source":"# 2. Extract: Data Collection\ndef extract_data(db):\n    try:\n        queries = {\n            'Products': \"SELECT * FROM Products\",\n            'Inventory': \"SELECT * FROM Inventory\",\n            'Sales': \"SELECT * FROM Sales\"\n        }\n\n        dataframes = {}\n        for name, query in queries.items():\n            dataframes[name] = pd.read_sql(query, db)\n            print(f\"Columns in {name} DataFrame: {dataframes[name].columns.tolist()}\")  # Debugging line\n        \n        return dataframes\n    except Exception as e:\n        print(f\"Error during data extraction: {e}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:20:39.003864Z","iopub.execute_input":"2024-11-12T10:20:39.004311Z","iopub.status.idle":"2024-11-12T10:20:39.012356Z","shell.execute_reply.started":"2024-11-12T10:20:39.004266Z","shell.execute_reply":"2024-11-12T10:20:39.011105Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"**3. Transform: Data Processing and Transformation**\ntransform_data(dataframes):\n\nIn this stage, the extracted data is processed and transformed into a format that is suitable for loading into the target database. The transformation involves data cleaning, merging, and calculating derived fields.\n\nSteps involved:\n\nMerging: Data from multiple sources (product_info, inventory, pricing, sales, replenishment) is merged into a single DataFrame using pandas merge() function. The key for merging is the product_id.\nCalculating Derived Fields:\ntotal_cost is calculated as inventory_qty * holding_cost_per_unit.\ntotal_profit is calculated as total_sales_value - total_cost.\navg_monthly_sales is calculated as total_sales_value / 12 (assuming monthly breakdown over 12 months).\nHandling Missing Data: The fillna(0) method is used to replace NaN values with zeros.\nDate Formatting: The inventory_date and replenishment_date columns are converted to the format YYYY-MM-DD.","metadata":{}},{"cell_type":"code","source":"\n\n\n\ndef transform_data(dataframes):\n    try:\n        # Ensure 'product_id' exists in all dataframes\n        for df_name, df in dataframes.items():\n            if 'product_id' not in df.columns:\n                print(f\"Warning: 'product_id' missing in {df_name} DataFrame\")\n        \n        # Merge all dataframes on 'product_id'\n        merged_df = dataframes['Products']\n        \n        # Merge Inventory Data\n        merged_df = merged_df.merge(dataframes['Inventory'], on='product_id', how='left')\n        \n        # Merge Sales Data\n        merged_df = merged_df.merge(dataframes['Sales'], on='product_id', how='left')\n        \n        # If 'season' and 'profit_margin_pct' columns are missing, add them with default values\n        if 'season' not in merged_df.columns:\n            merged_df['season'] = 'Unknown'  # Default value\n        if 'profit_margin_pct' not in merged_df.columns:\n            merged_df['profit_margin_pct'] = 0.0  # Default value\n        \n        # Calculate derived fields\n        merged_df['inventory_value'] = merged_df['inventory_qty'] * merged_df['original_price_per_unit']\n        merged_df['total_sales_value'] = merged_df['items_sold'] * merged_df['sale_price_per_unit']\n        merged_df['total_profit'] = merged_df['total_sales_value'] - merged_df['inventory_value']\n        \n        # Calculate averages\n        merged_df['avg_items_sold_per_month'] = merged_df['items_sold'] / 12\n        merged_df['avg_sales_revenue_per_month'] = merged_df['total_sales_value'] / 12\n        merged_df['avg_profit_per_month'] = merged_df['total_profit'] / 12\n        \n        # Clean missing data by filling NaN values with zero\n        merged_df.fillna(0, inplace=True)\n        \n        # Convert 'inventory_date' to proper date format\n        merged_df['inventory_date'] = pd.to_datetime(merged_df['inventory_date']).dt.strftime('%Y-%m-%d')\n        \n        # Ensure 'lead_time_days' is an integer and replace NaN with 0\n        merged_df['lead_time_days'] = merged_df['lead_time_days'].fillna(0).astype(int)\n        \n        # Filter columns to match the target structure (only the needed columns)\n        merged_df = merged_df[['product_id', 'product_name', 'inventory_date', 'inventory_qty', \n                               'original_price_per_unit', 'inventory_value', 'sale_price_per_unit', \n                               'items_sold', 'total_sales_value', 'total_profit', \n                               'avg_items_sold_per_month', 'avg_sales_revenue_per_month', \n                               'avg_profit_per_month', 'warehouse_location', 'lead_time_days', 'season']]\n        \n        return merged_df\n    except Exception as e:\n        print(f\"Error during data transformation: {e}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:21:45.649691Z","iopub.execute_input":"2024-11-12T10:21:45.650190Z","iopub.status.idle":"2024-11-12T10:21:45.665183Z","shell.execute_reply.started":"2024-11-12T10:21:45.650148Z","shell.execute_reply":"2024-11-12T10:21:45.663828Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"**3. Load: Loading the Transformed Data into Target Database**\nload_data(db, cursor, merged_df):\n\nIn this step, the transformed data is loaded into a target database (in this case, the same MySQL database but into a new table called TransformedProductData).\n\nSteps involved:\n\nTruncate Existing Data: The table TransformedProductData is cleared using TRUNCATE TABLE to ensure there are no old records.\nPrepare Insert Query: The SQL insert query is prepared with placeholders (%s) for each value to be inserted into the table.\nInserting Data: Data from the transformed DataFrame (merged_df) is inserted row by row into the TransformedProductData table. Each row is converted into a tuple of values that corresponds to the table columns.\nCommit the Transaction: The commit() method ensures that the changes are saved to the database.\nError Handling: In case of an error, a rollback() is triggered to undo any changes made during the process.","metadata":{}},{"cell_type":"code","source":"\n# 4. Load: Loading the Transformed Data into Target Database\ndef load_data(db, cursor, merged_df):\n    try:\n        # Create table if it doesn't exist (only relevant columns)\n        create_table_query = \"\"\"\n        CREATE TABLE IF NOT EXISTS TransformedProductData (\n            product_id INT PRIMARY KEY,\n            product_name VARCHAR(255),\n            inventory_date DATE,\n            inventory_qty INT,\n            original_price_per_unit DECIMAL(10,2),\n            inventory_value DECIMAL(10,2),\n            sale_price_per_unit DECIMAL(10,2),\n            total_items_sold INT,\n            total_sales_value DECIMAL(10,2),\n            total_profit DECIMAL(10,2),\n            avg_items_sold_per_month DECIMAL(10,2),\n            avg_sales_revenue_per_month DECIMAL(10,2),\n            avg_profit_per_month DECIMAL(10,2),\n            warehouse_location VARCHAR(255),\n            lead_time_days INT,\n            season VARCHAR(50)\n        );\n        \"\"\"\n        cursor.execute(create_table_query)\n        \n        # Optionally truncate the table before loading new data\n        cursor.execute(\"TRUNCATE TABLE TransformedProductData;\")\n        \n        # Make sure the number of columns and placeholders match\n        insert_query = \"\"\"\n        INSERT INTO TransformedProductData (\n            product_id, product_name, inventory_date, inventory_qty, original_price_per_unit, \n            inventory_value, sale_price_per_unit, total_items_sold, total_sales_value, total_profit,\n            avg_items_sold_per_month, avg_sales_revenue_per_month, avg_profit_per_month, warehouse_location, \n            lead_time_days, season\n        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n        \"\"\"\n        \n        # Insert data into the table\n        for _, row in merged_df.iterrows():\n            values = (\n                row['product_id'], row['product_name'], row['inventory_date'], row['inventory_qty'],\n                row['original_price_per_unit'], row['inventory_value'], row['sale_price_per_unit'],\n                row['items_sold'], row['total_sales_value'], row['total_profit'],\n                row['avg_items_sold_per_month'], row['avg_sales_revenue_per_month'],\n                row['avg_profit_per_month'], row['warehouse_location'], row['lead_time_days'], row['season']\n            )\n            cursor.execute(insert_query, values)\n        \n        db.commit()\n        print(f\"Successfully loaded {len(merged_df)} records into the database\")\n    except Exception as e:\n        db.rollback()\n        print(f\"Error during data loading: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-12T10:21:49.264018Z","iopub.execute_input":"2024-11-12T10:21:49.264444Z","iopub.status.idle":"2024-11-12T10:21:49.276319Z","shell.execute_reply.started":"2024-11-12T10:21:49.264404Z","shell.execute_reply":"2024-11-12T10:21:49.274879Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"**Putting It All Together**\n\nThe main function orchestrates the entire ETL process. It:\n\nConnects to the MySQL Database using create_db_connection().\nExtracts Data from the source tables with extract_data().\nTransforms the Data using transform_data() (merging, calculating new fields, and cleaning the data).\nCreates a Target Table if it doesn't exist with create_target_table().\nLoads the Transformed Data into the target table with load_data().\nHandles exceptions and ensures that connections are properly closed at the end of the process.","metadata":{}},{"cell_type":"code","source":"\n# Main ETL Process\ndef main():\n    try:\n        db = create_db_connection()\n        if not db:\n            return\n        \n        cursor = db.cursor()\n        \n        print(\"Extracting data...\")\n        dataframes = extract_data(db)\n        if not dataframes:\n            return\n        \n        print(\"Transforming data...\")\n        merged_df = transform_data(dataframes)\n        if merged_df is None:\n            return\n        \n        print(\"Loading data...\")\n        load_data(db, cursor, merged_df)\n        \n        print(\"ETL process completed successfully!\")\n    \n    except Exception as e:\n        print(f\"ETL process failed: {e}\")\n    finally:\n        if 'cursor' in locals():\n            cursor.close()\n        if 'db' in locals():\n            db.close()\n            print(\"Database connection closed.\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T12:20:40.692474Z","iopub.execute_input":"2024-11-12T12:20:40.693048Z","iopub.status.idle":"2024-11-12T12:20:48.564519Z","shell.execute_reply.started":"2024-11-12T12:20:40.692993Z","shell.execute_reply":"2024-11-12T12:20:48.563445Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Extracting data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_30/3616781678.py:32: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  dataframes[name] = pd.read_sql(query, db)\n","output_type":"stream"},{"name":"stdout","text":"Columns in Products DataFrame: ['product_id', 'product_name', 'original_price_per_unit', 'sale_price_per_unit', 'lead_time_days', 'seasonal_sales']\nColumns in Inventory DataFrame: ['inventory_id', 'product_id', 'inventory_date', 'inventory_qty', 'inventory_value', 'warehouse_location']\nColumns in Sales DataFrame: ['sales_id', 'product_id', 'sales_date', 'items_sold', 'sales_value', 'profit']\nTransforming data...\nLoading data...\nSuccessfully loaded 14 records into the database\nETL process completed successfully!\nDatabase connection closed.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Summary of the ETL Process**\nExtract: Data is fetched from multiple MySQL tables using SQL queries.\nTransform: Data is processed to create new metrics, cleaned, and merged into a single DataFrame.\nLoad: The transformed data is inserted into a new table in the MySQL database after truncating any existing data.\nThe ETL process automates the movement and transformation of data to make it useful for analysis and reporting. It ensures that the target table contains up-to-date, clean, and comprehensive data for business intelligence.","metadata":{}}]}